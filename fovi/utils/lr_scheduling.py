import torch
import math
import warnings
import numpy as np
from . import add_to_all

FALSY_STRINGS = {"off", "false", "0"}
TRUTHY_STRINGS = {"on", "true", "1"}

__all__ = []

@add_to_all(__all__)
class LARS(torch.optim.Optimizer):
    """Layer-wise Adaptive Rate Scaling (LARS) optimizer.
    
    LARS is designed for training with large batch sizes by scaling the learning
    rate layer-wise based on the ratio of parameter norm to gradient norm.
    
    Based on: https://github.com/facebookresearch/barlowtwins/blob/main/main.py
    
    Args:
        params: Iterable of parameters to optimize.
        lr (float, optional): Base learning rate. Defaults to 0.
        weight_decay (float, optional): Weight decay coefficient. Defaults to 0.
        momentum (float, optional): Momentum factor. Defaults to 0.9.
        eta (float, optional): LARS coefficient for adaptive scaling. Defaults to 0.001.
        weight_decay_filter (callable, optional): Filter for weight decay. Defaults to None.
        lars_adaptation_filter (callable, optional): Filter for LARS adaptation. Defaults to None.
    """
    def __init__(self, params, lr=0, weight_decay=0, momentum=0.9, eta=0.001,
                 weight_decay_filter=None, lars_adaptation_filter=None):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum,
                        eta=eta, weight_decay_filter=weight_decay_filter,
                        lars_adaptation_filter=lars_adaptation_filter)
        super().__init__(params, defaults)

    @torch.no_grad()
    def step(self):
        for g in self.param_groups:
            for p in g['params']:
                dp = p.grad

                if dp is None:
                    continue

                if p.ndim != 1:
                    dp = dp.add(p, alpha=g['weight_decay'])

                if p.ndim != 1:
                    param_norm = torch.norm(p)
                    update_norm = torch.norm(dp)
                    one = torch.ones_like(param_norm)
                    q = torch.where(param_norm > 0.,
                                    torch.where(update_norm > 0,
                                                (g['eta'] * param_norm / update_norm), one), one)
                    dp = dp.mul(q)

                param_state = self.state[p]
                if 'mu' not in param_state:
                    param_state['mu'] = torch.zeros_like(p)
                mu = param_state['mu']
                mu.mul_(g['momentum']).add_(dp)

                p.add_(mu, alpha=-g['lr'])

def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


@add_to_all(__all__)
def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    """Fill a tensor with values from a truncated normal distribution.
    
    Values are drawn from N(mean, std) and truncated to [a, b].
    
    Args:
        tensor (torch.Tensor): Tensor to fill in-place.
        mean (float, optional): Mean of the normal distribution. Defaults to 0.
        std (float, optional): Standard deviation. Defaults to 1.
        a (float, optional): Minimum truncation bound. Defaults to -2.
        b (float, optional): Maximum truncation bound. Defaults to 2.
        
    Returns:
        torch.Tensor: The filled tensor.
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


@add_to_all(__all__)
def cosine_decay(global_step, max_steps, initial_value, end_value):
    """Compute value at a step using cosine annealing schedule.
    
    Smoothly decays from initial_value to end_value following a cosine curve.
    
    Args:
        global_step (int): Current step.
        max_steps (int): Total number of steps for the schedule.
        initial_value (float): Starting value.
        end_value (float): Final value.
        
    Returns:
        float: The interpolated value at the current step.
    """
    global_step = min(global_step, max_steps)
    cosine_decay_value = 0.5 * (1 + math.cos(math.pi * global_step / max_steps))
    return (initial_value - end_value) * cosine_decay_value + end_value


@add_to_all(__all__)
def cosine_scheduler(base_value, final_value, epochs, niter_per_ep, warmup_epochs=0, start_warmup_value=0):
    """Create a full cosine annealing schedule with optional warmup.
    
    Returns an array of values for each iteration across all epochs,
    with optional linear warmup at the beginning.
    
    Args:
        base_value (float): Peak value after warmup.
        final_value (float): Final value at end of training.
        epochs (int): Total number of epochs.
        niter_per_ep (int): Number of iterations per epoch.
        warmup_epochs (int, optional): Number of warmup epochs. Defaults to 0.
        start_warmup_value (float, optional): Starting value for warmup. Defaults to 0.
        
    Returns:
        np.ndarray: Array of length (epochs * niter_per_ep) with scheduled values.
    """
    warmup_schedule = np.array([])
    warmup_iters = warmup_epochs * niter_per_ep
    if warmup_epochs > 0:
        warmup_schedule = np.linspace(start_warmup_value, base_value, warmup_iters)

    iters = np.arange(epochs * niter_per_ep - warmup_iters)
    schedule = final_value + 0.5 * (base_value - final_value) * (1 + np.cos(np.pi * iters / len(iters)))

    schedule = np.concatenate((warmup_schedule, schedule))
    assert len(schedule) == epochs * niter_per_ep
    return schedule


@add_to_all(__all__)
def learning_schedule(
    global_step, batch_size, base_lr, end_lr_ratio, total_steps, warmup_steps
):
    """Compute learning rate at a step with batch size scaling and warmup.
    
    Scales the base learning rate linearly with batch size (relative to 256),
    applies linear warmup, then cosine decay.
    
    Args:
        global_step (int): Current training step.
        batch_size (int): Batch size for linear scaling.
        base_lr (float): Base learning rate (for batch size 256).
        end_lr_ratio (float): Ratio of final LR to scaled base LR.
        total_steps (int): Total number of training steps.
        warmup_steps (int): Number of warmup steps.
        
    Returns:
        float: Learning rate for the current step.
    """
    scaled_lr = base_lr * batch_size / 256.0
    end_lr = scaled_lr * end_lr_ratio
    learning_rate = (
        global_step / warmup_steps * scaled_lr if warmup_steps > 0 else scaled_lr
    )
    if global_step < warmup_steps:
        return learning_rate
    else:
        return cosine_decay(
            global_step - warmup_steps, total_steps - warmup_steps, scaled_lr, end_lr
        )


@add_to_all(__all__)
class CosineDecayWithWarmup():
    """Callable learning rate schedule with cosine decay and linear warmup.
    
    Scales the base learning rate by batch size, applies linear warmup,
    then cosine annealing to the end learning rate.
    
    Args:
        batch_size (int): Batch size for linear scaling (relative to 256).
        base_lr (float): Base learning rate (for batch size 256).
        end_lr_ratio (float): Ratio of final LR to scaled base LR.
        total_steps (int): Total number of training steps.
        warmup_steps (int): Number of warmup steps.
        
    Attributes:
        scaled_base_lr (float): Base LR scaled by batch size.
        end_lr (float): Final learning rate.
    """
    def __init__(self, batch_size, base_lr, end_lr_ratio, total_steps, warmup_steps):
        self.batch_size = batch_size
        self.base_lr = base_lr
        self.end_lr_ratio = end_lr_ratio
        self.total_steps = total_steps
        self.warmup_steps = warmup_steps
        self.scaled_base_lr = base_lr*batch_size / 256.0
        self.end_lr = self.scaled_base_lr * self.end_lr_ratio
        
    def __call__(self, step):
        if step < self.warmup_steps:
            lr = (step / self.warmup_steps)*self.scaled_base_lr
        else:
            lr = cosine_decay(step - self.warmup_steps, self.total_steps - self.warmup_steps, self.scaled_base_lr, self.end_lr)
        return lr

    def state_dict(self):
        return None
    
    def load_state_dict(self, state_dict):
        pass